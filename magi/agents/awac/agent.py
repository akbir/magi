"""AWAC agent implementation"""
import dataclasses
from typing import Callable, Dict, Iterator, List, Optional

from acme import adders
from acme import core
from acme import datasets
from acme import specs
from acme.adders import reverb as adders_reverb
from acme.agents import agent
from acme.agents.jax import actors as acting_lib
from acme.jax import networks as networks_lib
from acme.jax import types as jax_types
from acme.jax import variable_utils
from acme.utils import counting
from acme.utils import loggers
import jax
import optax
import reverb
from reverb import rate_limiters

from magi.agents.awac import learning as learning_lib

Networks = Dict[str, Callable]
PolicyNetwork = Callable
Sample = reverb.ReplaySample


@dataclasses.dataclass
class AWACConfig:
    replay_table_name: str = adders_reverb.DEFAULT_PRIORITY_TABLE
    min_replay_size: int = 1
    max_replay_size: int = 1_000_000
    prefetch_size: Optional[int] = None
    batch_size: int = 256

    policy_optimizer: Optional[optax.GradientTransformation] = None
    critic_optimizer: Optional[optax.GradientTransformation] = None

    discount: float = 0.99
    tau: float = 0.005
    target_update_period: int = 2
    beta: float = 1.0
    num_samples: int = 1


class AWACBuilder:
    """Soft Actor-Critic agent specification"""

    def __init__(self, config: AWACConfig):
        self._config = config

    def make_replay_tables(
        self,
        environment_spec: specs.EnvironmentSpec,
    ) -> List[reverb.Table]:
        """Create tables to insert data into."""
        replay_table = reverb.Table(
            name=self._config.replay_table_name,
            # TODO(yl): support prioritized sampling
            sampler=reverb.selectors.Uniform(),
            remover=reverb.selectors.Fifo(),
            max_size=self._config.max_replay_size,
            rate_limiter=rate_limiters.MinSize(self._config.min_replay_size),
            signature=adders_reverb.NStepTransitionAdder.signature(
                environment_spec=environment_spec
            ),
        )
        return [replay_table]

    def make_dataset_iterator(
        self,
        replay_client: reverb.Client,
    ) -> Iterator[Sample]:
        """Create a dataset iterator to use for learning/updating the agent."""
        dataset = datasets.make_reverb_dataset(
            table=self._config.replay_table_name,
            server_address=replay_client.server_address,
            batch_size=self._config.batch_size,
            prefetch_size=self._config.prefetch_size,
            transition_adder=True,
        )
        return dataset.as_numpy_iterator()

    def make_adder(
        self,
        replay_client: reverb.Client,
    ) -> Optional[adders.Adder]:
        """Create an adder which records data generated by the actor/environment.

        Args:
          replay_client: Reverb Client which points to the replay server.
        """
        # TODO(yl): support multi step transitions
        return adders_reverb.NStepTransitionAdder(
            client=replay_client, n_step=1, discount=self._config.discount
        )

    def make_actor(
        self,
        random_key: networks_lib.PRNGKey,
        policy_network: PolicyNetwork,
        adder: Optional[adders.Adder] = None,
        variable_source: Optional[core.VariableSource] = None,
    ) -> core.Actor:
        """Create an actor instance."""
        assert variable_source is not None
        variable_client = variable_utils.VariableClient(variable_source, "")
        variable_client.update_and_wait()

        return acting_lib.FeedForwardActor(
            policy_network, random_key, variable_client=variable_client, adder=adder
        )

    def make_learner(
        self,
        environment_spec: specs.EnvironmentSpec,
        random_key: networks_lib.PRNGKey,
        networks: Networks,
        dataset: Iterator[Sample],
        logger: Optional[loggers.Logger] = None,
        counter: Optional[counting.Counter] = None,
    ) -> core.Learner:
        """Create a learner instance."""
        return learning_lib.AWACLearner(
            environment_spec=environment_spec,
            policy_network=networks["policy"],
            critic_network=networks["critic"],
            iterator=dataset,
            random_key=random_key,
            policy_optimizer=self._config.policy_optimizer,
            critic_optimizer=self._config.critic_optimizer,
            discount=self._config.discount,
            tau=self._config.tau,
            target_update_period=self._config.target_update_period,
            beta=self._config.beta,
            num_samples=self._config.num_samples,
            logger=logger,
            counter=counter,
        )


class AWACAgent(agent.Agent):
    def __init__(
        self,
        environment_spec: specs.EnvironmentSpec,
        networks: Networks,
        random_key: jax_types.PRNGKey,
        config: AWACConfig,
        min_observations: Optional[int] = None,
        counter: Optional[counting.Counter] = None,
        logger: Optional[loggers.Logger] = None,
    ):
        builder = AWACBuilder(config)
        if min_observations is None:
            min_observations = max(config.batch_size, config.min_replay_size)
        learner_key, actor_key = jax.random.split(random_key)

        replay_tables = builder.make_replay_tables(environment_spec)
        replay_server = reverb.Server(replay_tables, port=None)
        replay_client = reverb.Client(f"localhost:{replay_server.port}")

        # Create actor, dataset, and learner for generating, storing, and consuming
        # data respectively.
        dataset = builder.make_dataset_iterator(replay_client)
        learner = builder.make_learner(
            environment_spec,
            networks,
            learner_key,
            dataset,
            counter=counter,
            logger=logger,
        )
        adder = builder.make_adder(replay_client)

        def behavior_policy(params, key, obs):
            action_dist = networks["policy"].apply(params, obs)
            return action_dist.sample(seed=key)

        actor = builder.make_actor(
            actor_key, behavior_policy, adder=adder, variable_source=learner
        )

        super().__init__(
            actor,
            learner,
            min_observations=min_observations,
            observations_per_step=1,
        )

        self.builder = builder
        self._replay_server = replay_server
